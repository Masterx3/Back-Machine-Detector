## This script was auto-generated by using
## % comet init

## For more information, please see:
## https://www.comet.ml/docs/command-line/#comet-init

import comet_ml
from comet_ml import ConfusionMatrix

import numpy as np
import random
import logging
LOGGER = logging.getLogger("comet_ml")
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
import keract


def get_comet_experiment(**args):
    return comet_ml.OfflineExperiment(**args)

def train(experiment, model, datasets):
    from torch.utils.data import SubsetRandomSampler

    rnn, criterion, optimizer = model
    train_dataset, test_dataset = datasets

    # Data Loader (Input Pipeline)
    # Sampling only from first 1000
    sampler = SubsetRandomSampler(list(range(1000)))
    train_loader = torch.utils.data.DataLoader(
        dataset=train_dataset,
        batch_size=experiment.get_parameter('batch_size'),
        sampler=sampler,
        #shuffle=True,
    )
    def index_to_example(index):
        image_array, label, _index = train_dataset[index]
        image_name = "confusion-matrix-%05d.png" % index
        result = experiment.log_image(
            image_array,
            name=image_name,
            image_shape=(28, 28, 1))
        #Return sample name and assetId
        return {"sample": image_name, "assetId": result["imageId"]}

    # We make a ConfusionMatrix here so that we can share
    # examples across epochs and cut down on the images
    # uploaded:
    confusion_matrix = ConfusionMatrix(
        index_to_example_function=index_to_example)

    # In order:
    print("Collecting 1000 data...")
    all_labels = [label for (image, label, index) in train_dataset if index < 1000]
    inputs = [image.numpy() for (image, label, index) in train_dataset if index < 1000]
    sprite_url = None
    with experiment.train():
        step = 0
        for epoch in range(experiment.get_parameter('epochs')):
            correct = 0
            total = 0
            batch_actual = []
            batch_predicted = []
            batch_indices = []
            for batch_step, (images, labels, indices) in enumerate(train_loader):
                print("batch step:", batch_step, "...")
                images = Variable(images.view(
                    -1,
                    experiment.get_parameter('sequence_length'),
                    experiment.get_parameter("input_size")))

                labels = Variable(labels)

                # Forward + Backward + Optimize
                optimizer.zero_grad()
                outputs = rnn(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                # Compute train accuracy
                _, predicted = torch.max(outputs.data, 1)
                batch_total = labels.size(0)
                total += batch_total

                batch_correct = (predicted == labels.data).sum()
                correct += batch_correct

                batch_predicted.extend(predicted.numpy().tolist())
                batch_actual.extend(labels.data.numpy().tolist())
                batch_indices.extend(indices.numpy().tolist())

                # Log batch_accuracy to Comet.ml; step is each batch
                step += 1
                experiment.log_metric("batch_accuracy", batch_correct / batch_total, step=step)

                if (batch_step + 1) % 100 == 0:
                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'
                          % (epoch + 1,
                             experiment.get_parameter('epochs'),
                             batch_step + 1,
                             len(train_dataset) // experiment.get_parameter('batch_size'),
                             loss.item()))

            # Log epoch accuracy to Comet.ml; step is each epoch
            experiment.log_metric("batch_accuracy", correct / total, step=epoch, epoch=epoch)

            indices, actual, predicted = list(zip(*sorted(zip(
                batch_indices, batch_actual, batch_predicted))))

            print("Creating confusion matrix...")
            confusion_matrix.compute_matrix(actual, predicted)

            experiment.log_confusion_matrix(
                matrix=confusion_matrix,
                title="MNIST Confusion Matrix, Epoch #%d" % (epoch + 1),
                file_name="confusion-matrix-%03d.json" % (epoch + 1),
            )

            # Epoch level items

            if epoch == 0:
                # First, create the embedding image:
                def label_to_color(index):
                    label = all_labels[index]
                    if label == 0:
                        return (255, 0, 0)
                    elif label == 1:
                        return (0, 255, 0)
                    elif label == 2:
                        return (0, 0, 255)
                    elif label == 3:
                        return (255, 255, 0)
                    elif label == 4:
                        return (0, 255, 255)
                    elif label == 5:
                        return (128, 128, 0)
                    elif label == 6:
                        return (0, 128, 128)
                    elif label == 7:
                        return (128, 0, 128)
                    elif label == 8:
                        return (255, 0, 255)
                    elif label == 9:
                        return (255, 255, 255)

                print("Creating embedding image...")
                results = experiment.create_embedding_image(
                    image_data=inputs,
                    # we round the pixels to 0 and 1, and multiple by 2
                    # to keep the non-zero colors dark (if they were 1, they
                    # would get distributed between 0 and 255):
                    image_preprocess_function=lambda matrix: np.round(matrix,0) * 2,
                    # Set the transparent color:
                    image_transparent_color=(0, 0, 0),
                    image_size=(
                        experiment.get_parameter("input_size"),
                        experiment.get_parameter("input_size"),
                    ),
                    # Fill in the transparent color with a background color:
                    image_background_color_function=label_to_color,
                )
                if results:
                    image, sprite_url = results
                else:
                    sprite_url = None
            else:
                # In subsequent epochs, log the activations:
                if sprite_url:
                    print("Logging embeddings...")
                    activations = []
                    for i in range(len(all_labels)):
                        v = Variable(train_dataset[i][0])
                        output = rnn(v)
                        activations.append(rnn.out[0].detach().numpy())
                    layer_name = "activations/0"
                    experiment.log_embedding(
                        activations,
                        all_labels,
                        image_data=sprite_url,
                        image_size=(
                            experiment.get_parameter("input_size"),
                            experiment.get_parameter("input_size"),
                        ),
                        title="%s-%s" % (layer_name, epoch + 1),
                        group=layer_name)
            # TODO: log weight histograms here!
            # TODO: log activation histograms here!
            # TODO: log gradient histograms here!


def build_model(experiment):
    input_size = experiment.get_parameter("input_size")
    hidden_size = experiment.get_parameter("hidden_size")
    num_layers = experiment.get_parameter("num_layers")
    num_classes = experiment.get_parameter("num_classes")
    learning_rate = experiment.get_parameter("learning_rate")

    class RNN(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes):
            super(RNN, self).__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            # Set initial states
            h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))
            c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))

            # Forward propagate RNN
            self.out, _ = self.lstm(x, (h0, c0))

            # Decode hidden state of last time step
            out = self.fc(self.out[:, -1, :])
            return out

    rnn = RNN(
        input_size,
        hidden_size,
        num_layers,
        num_classes,
    )

    # Loss and Optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)

    return (rnn, criterion, optimizer)


def evaluate(experiment, model, datasets):
    rnn, criterion, optimizer = model
    train_dataset, test_dataset = datasets

    test_loader = torch.utils.data.DataLoader(
        dataset=test_dataset,
        batch_size=experiment.get_parameter('batch_size'),
        shuffle=False)

    with experiment.test():
        # Test the Model
        correct = 0
        total = 0
        for images, labels, index in test_loader:
            images = Variable(images.view(
                -1,
                experiment.get_parameter('sequence_length'),
                experiment.get_parameter("input_size")))
            outputs = rnn(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()

        experiment.log_metric("accuracy", correct / total)
        print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))


def get_datasets(hyper_params):
    # MNIST Dataset
    def indexed_dataset(cls):
        def __getitem__(self, index):
            data, target = cls.__getitem__(self, index)
            return data, target, index

        return type(cls.__name__, (cls,), {
            '__getitem__': __getitem__,
        })

    MNIST = indexed_dataset(dsets.MNIST)

    train_dataset = MNIST(root='./data/',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

    test_dataset = MNIST(root='./data/',
                         train=False,
                         transform=transforms.ToTensor())

    return (train_dataset, test_dataset)


def main():
    hyper_params = {
        "epochs": 10,
        "batch_size": 120,
        "first_layer_units": 128,
        "sequence_length": 28,
        "input_size": 28,
        "hidden_size": 128,
        "num_layers": 2,
        "num_classes": 10,
        "learning_rate": 0.01
    }

    experiment_kwargs = {
        "project_name": "my_project",
        "offline_directory": ".",
        "auto_histogram_weight_logging": True,
        "auto_histogram_gradient_logging": True,
        "auto_histogram_activation_logging": True,
    }

    datasets = get_datasets(hyper_params)
    config = {
        "algorithm": "bayes",
        "name": "Optimize MNIST Network",
        "spec": {
            "maxCombo": 10,
            "objective":
            "minimize",
            "metric": "train_loss",
        },
        "parameters": {
            "first_layer_units": {
                "type": "integer",
                "mu": 500,
                "sigma": 50,
                "scalingType": "normal",
            },
            "batch_size": {"type": "discrete", "values": [64, 128, 256]},
        },
        "trials": 1,
    }
    experiment_class = comet_ml.OfflineExperiment

    opt = comet_ml.Optimizer(config, experiment_class=experiment_class)

    for experiment in opt.get_experiments(**experiment_kwargs):
        # We remove the two hyperparameters that are set by optimizer
        # as we don't want to overwrite them:
        del hyper_params["first_layer_units"]
        del hyper_params["batch_size"]
        # Log the remainder:
        experiment.log_parameters(hyper_params)

        model = build_model(experiment)

        train(experiment, model, datasets)

        evaluate(experiment, model, datasets)

        experiment.end()


if __name__ == "__main__":
    main()
